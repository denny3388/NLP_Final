{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "bert-utter",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T17:36:58.269568Z",
          "iopub.execute_input": "2022-06-03T17:36:58.270149Z",
          "iopub.status.idle": "2022-06-03T17:37:08.993566Z",
          "shell.execute_reply.started": "2022-06-03T17:36:58.270113Z",
          "shell.execute_reply": "2022-06-03T17:37:08.992416Z"
        },
        "trusted": true,
        "id": "NXA2F10yEN7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "import time\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from IPython.display import clear_output\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import Dataset\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T17:37:08.996032Z",
          "iopub.execute_input": "2022-06-03T17:37:08.996501Z",
          "iopub.status.idle": "2022-06-03T17:37:09.004191Z",
          "shell.execute_reply.started": "2022-06-03T17:37:08.996458Z",
          "shell.execute_reply": "2022-06-03T17:37:09.003091Z"
        },
        "trusted": true,
        "id": "oBK8_01uEN7l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "OI8u2CzSF4Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### MOUNT THE FILE SYSTEM\n",
        "from google.colab import drive\n",
        "drive.mount('/content/BDA')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qZ4upc-FFgE",
        "outputId": "7a3665c9-0f85-4774-d4fc-15c5be0fda33"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/BDA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Data input \"\"\"\n",
        "\n",
        "train = pd.read_csv('/content/BDA/MyDrive/dataset/fixed_train.csv')\n",
        "valid = pd.read_csv('/content/BDA/MyDrive/dataset/fixed_valid.csv')\n",
        "test = pd.read_csv('/content/BDA/MyDrive/dataset/fixed_test.csv')\n",
        "# train.head(20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T17:37:09.005818Z",
          "iopub.execute_input": "2022-06-03T17:37:09.006640Z",
          "iopub.status.idle": "2022-06-03T17:37:09.291960Z",
          "shell.execute_reply.started": "2022-06-03T17:37:09.006602Z",
          "shell.execute_reply": "2022-06-03T17:37:09.291179Z"
        },
        "trusted": true,
        "id": "pCgH0SZEEN7l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Data preprocessing \"\"\"\n",
        "\n",
        "# 取得此預訓練模型所使用的 tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "class myDataset(Dataset):\n",
        "    # 讀取前處理後的 csv 檔並初始化一些參數\n",
        "    def __init__(self, mode, df, tokenizer):\n",
        "        assert mode in [\"train\", \"valid\", \"test\"]\n",
        "        self.mode = mode\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "        # df = df[df['utterance_idx']%2 == 1].copy()\n",
        "        # df['utterance'] = df.groupby(['prompt'])['utterance'].transform(lambda x: ' '.join(x))\n",
        "        # df['utterance'] = df['utterance'].apply(nltk.sent_tokenize)\n",
        "        # df = df.explode('utterance')\n",
        "        df = df.drop('conv_id', axis = 1)\n",
        "        df = df.drop('utterance_idx', axis = 1)\n",
        "#         df = df.drop('utterance', axis = 1)\n",
        "        \n",
        "        self.df = df.dropna()\n",
        "        if self.mode != \"test\":\n",
        "            self.df.drop_duplicates(inplace=True)\n",
        "        self.len = len(self.df)\n",
        "        self.maxlen = 100\n",
        "        self.white_list = [\"don't\", \"not\", \"didn't\", \"cannot\", \"can't\"]\n",
        "        self.nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
        "        self.punc = '''.()-[]{};:\"\\,<>/@#$%^&*_~”'''\n",
        "        self.ps = PorterStemmer()\n",
        "    \n",
        "    # Preprocess token\n",
        "    def cleardata(self, ss):\n",
        "        token = tokenizer.tokenize(ss.lower().strip().replace('_comma_',' '))\n",
        "        \n",
        "        # token = [word for word in token if (word not in self.nltk_stopwords) or (word in self.white_list)]\n",
        "        \n",
        "        # token = [self.ps.stem(w) for w in token if w not in self.white_list]\n",
        "        \n",
        "        token = [w for w in token if w not in self.punc] \n",
        "        \n",
        "        return token\n",
        "    \n",
        "    # 這裡需要定義回傳一筆訓練 / 測試數據的函式，\n",
        "    # 也就是當以 [idx] 來 index Dataset 時，要回傳的東西\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == \"test\":\n",
        "            prompt = self.df.iloc[idx]['prompt']\n",
        "            utter = self.df.iloc[idx]['utterance']\n",
        "            label_tensor = None\n",
        "        else:\n",
        "            prompt = self.df.iloc[idx]['prompt']\n",
        "            utter = self.df.iloc[idx]['utterance']\n",
        "            label = self.df.iloc[idx]['label']\n",
        "            # 將 label 文字也轉換成索引方便轉換成 tensor，\n",
        "            label_tensor = torch.tensor(label)\n",
        "            \n",
        "        # [TODO1]: 將 input sentence 轉成 [CLS] + text_a + [SEP] + text_b + [SEP]\n",
        "        # ==============================================\n",
        "        word_pieces = ['[CLS]']\n",
        "        word_pieces += self.cleardata(prompt)\n",
        "        word_pieces += ['[SEP]']\n",
        "        word_pieces += self.cleardata(utter)\n",
        "        word_pieces += ['[SEP]']\n",
        "        \n",
        "        # ==============================================\n",
        "        \n",
        "        # 將剛剛做好的 input sentence 轉換成索引序列\n",
        "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
        "        ids = [id for id in ids if id != 100]\n",
        "        ids = ids[:self.maxlen]\n",
        "        tokens_tensor = torch.tensor(ids)\n",
        "        \n",
        "        # [TODO2]: 將第一句+ [SEP] 的 token 位置設為 0，其他為 1 來表示句子間的segment\n",
        "        # ==============================================\n",
        "        seg = 0\n",
        "        cut_points = (tokens_tensor == 102).nonzero().flatten()\n",
        "        segments_tensor = torch.zeros(tokens_tensor.shape, dtype=torch.long)\n",
        "        if len(cut_points)>0:\n",
        "            segments_tensor[cut_points[0]+1:] = 1\n",
        "        # ==============================================\n",
        "        \n",
        "        return (tokens_tensor, segments_tensor, label_tensor)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "# 初始化 Dataset，使用中文 BERT 斷詞\n",
        "trainset = myDataset(\"train\", train, tokenizer=tokenizer)\n",
        "validset = myDataset(\"valid\", valid, tokenizer=tokenizer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:26:04.903773Z",
          "iopub.execute_input": "2022-06-03T18:26:04.904159Z",
          "iopub.status.idle": "2022-06-03T18:26:05.585014Z",
          "shell.execute_reply.started": "2022-06-03T18:26:04.904127Z",
          "shell.execute_reply": "2022-06-03T18:26:05.584177Z"
        },
        "trusted": true,
        "id": "evLKtdeUEN7m"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 選擇第一個樣本出來，看看原始 input 是怎麼被轉換成 BERT 相容的格式的\n",
        "sample_idx = 0\n",
        "\n",
        "# 將原始文本拿出做比較\n",
        "prompt, utter, label = trainset.df.iloc[sample_idx].values\n",
        "\n",
        "# 利用剛剛建立的 Dataset 取出轉換後的 id tensors，\n",
        "# 經過我們自定義的 Dataset 後，trainset 現在已經是個 iterable 的 object，\n",
        "# 可以用編號來索引你想要去得的位置的 id tensors\n",
        "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
        "\n",
        "# 將 tokens_tensor 還原成文本\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
        "combined_text = \" \".join(tokens)\n",
        "\n",
        "print(f\"\"\"[原始文本]\n",
        "情境  ：{prompt}\n",
        "對話  ：{utter}\n",
        "分類  ：{label}\n",
        "\n",
        "--------------------\n",
        "\n",
        "[Dataset 回傳的 tensors]\n",
        "tokens_tensor  ：{tokens_tensor}\n",
        "\n",
        "segments_tensor：{segments_tensor}\n",
        "\n",
        "label_tensor   ：{label_tensor}\n",
        "\n",
        "--------------------\n",
        "\n",
        "[還原 tokens_tensors]\n",
        "{combined_text}\n",
        "\"\"\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T17:37:10.181459Z",
          "iopub.execute_input": "2022-06-03T17:37:10.181888Z",
          "iopub.status.idle": "2022-06-03T17:37:10.194716Z",
          "shell.execute_reply.started": "2022-06-03T17:37:10.181852Z",
          "shell.execute_reply": "2022-06-03T17:37:10.193516Z"
        },
        "trusted": true,
        "id": "1D6I41BGEN7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# create_mini_batch 的參數 `samples` 是一個 list，裡頭的每個 element 都是\n",
        "# 剛剛定義的 `FakeNewsDataset` 回傳的一個樣本，每個樣本都包含 3 tensors：\n",
        "# - tokens_tensor\n",
        "# - segments_tensor\n",
        "# - label_tensor\n",
        "# 它會對前兩個 tensors 作 zero padding，並產生前面說明過的 masks_tensors\n",
        "def create_mini_batch(samples):\n",
        "    tokens_tensors = [s[0] for s in samples]\n",
        "    segments_tensors = [s[1] for s in samples]\n",
        "    \n",
        "    # 測試集有 labels\n",
        "    if samples[0][2] is not None:\n",
        "        label_ids = torch.stack([s[2] for s in samples])\n",
        "    else:\n",
        "        label_ids = None\n",
        "    \n",
        "    # [TODO3]: 將 token_tensors 及 segments_tensors zero padding 到同樣長度，\n",
        "    # hint: 可以使用 import的 pad_sequence，記得 batch_first 要設為 True\n",
        "    #================================================\n",
        "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
        "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
        "    #================================================\n",
        "\n",
        "\n",
        "    # [TODO4] 製作 attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
        "    # 的位置設為 1，讓 BERT 只關注這些位置的 tokens\n",
        "    # ================================================\n",
        "    # 先製作一條長度和 token_tensors 一樣的 0 張量\n",
        "    masks_tensors = torch.zeros(tokens_tensors.shape,dtype=torch.long)\n",
        "        \n",
        "    # 接著將不為 zero padding 的位置設為1 (由同學實作)\n",
        "    # hint: 可以使用 tensor.masked_fill()\n",
        "    for i in range(len(masks_tensors)):\n",
        "      for j in range(len(masks_tensors[i])):\n",
        "        if(tokens_tensors[i][j]==0):\n",
        "          break\n",
        "        masks_tensors[i][j] = 1\n",
        "    # ================================================\n",
        "    \n",
        "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
        "\n",
        "\n",
        "# 初始化一個每次回傳 BATCH_SIZE 個訓練樣本的 DataLoader\n",
        "# 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵\n",
        "trainloader = DataLoader(trainset, batch_size=128, \n",
        "                         collate_fn=create_mini_batch)\n",
        "\n",
        "validloader = DataLoader(validset, batch_size=256, \n",
        "                         collate_fn=create_mini_batch)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T17:37:10.196786Z",
          "iopub.execute_input": "2022-06-03T17:37:10.197281Z",
          "iopub.status.idle": "2022-06-03T17:37:10.217921Z",
          "shell.execute_reply.started": "2022-06-03T17:37:10.197239Z",
          "shell.execute_reply": "2022-06-03T17:37:10.216756Z"
        },
        "trusted": true,
        "id": "dwj_Zw4UEN7o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = next(iter(trainloader))\n",
        "\n",
        "tokens_tensors, segments_tensors, \\\n",
        "    masks_tensors, label_ids = data\n",
        "\n",
        "print(f\"\"\"\n",
        "tokens_tensors.shape   = {tokens_tensors.shape} \n",
        "{tokens_tensors}\n",
        "------------------------\n",
        "segments_tensors.shape = {segments_tensors.shape}\n",
        "{segments_tensors}\n",
        "------------------------\n",
        "masks_tensors.shape    = {masks_tensors.shape}\n",
        "{masks_tensors}\n",
        "------------------------\n",
        "label_ids.shape        = {label_ids.shape}\n",
        "{label_ids}\n",
        "\"\"\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T17:37:10.219509Z",
          "iopub.execute_input": "2022-06-03T17:37:10.220142Z",
          "iopub.status.idle": "2022-06-03T17:37:10.595870Z",
          "shell.execute_reply.started": "2022-06-03T17:37:10.220088Z",
          "shell.execute_reply": "2022-06-03T17:37:10.594647Z"
        },
        "trusted": true,
        "id": "KYqZdCgsEN7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "from IPython.display import clear_output\n",
        "\n",
        "NUM_LABELS = 32\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=NUM_LABELS)\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T17:37:10.597475Z",
          "iopub.execute_input": "2022-06-03T17:37:10.598110Z",
          "iopub.status.idle": "2022-06-03T17:37:12.272790Z",
          "shell.execute_reply.started": "2022-06-03T17:37:10.598064Z",
          "shell.execute_reply": "2022-06-03T17:37:12.271731Z"
        },
        "trusted": true,
        "id": "rWY1wKZdEN7q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from tqdm.notebook import tqdm\n",
        "def get_predictions(model, dataloader, compute_acc=False):\n",
        "    predictions = None\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(dataloader):\n",
        "            if next(model.parameters()).is_cuda:\n",
        "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
        "            \n",
        "            if not compute_acc:\n",
        "                # 只是單純要回傳預測值的話，不用計算準確度也不用紀錄 loss\n",
        "                tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
        "                outputs = model(input_ids=tokens_tensors, \n",
        "                                token_type_ids=segments_tensors, \n",
        "                                attention_mask=masks_tensors)\n",
        "                logits = outputs[0]\n",
        "                _, pred = torch.max(logits.data, 1)\n",
        "            else:\n",
        "                # 否則就要計算 loss，這邊有一個小細節是 model 如果有吃 label 的話，\n",
        "                # output[0]會變成是 loss，沒有吃 label 時 output[0] 會是 logits\n",
        "                tokens_tensors, segments_tensors, masks_tensors, labels = data[:4]\n",
        "                outputs = model(input_ids=tokens_tensors, \n",
        "                                token_type_ids=segments_tensors, \n",
        "                                attention_mask=masks_tensors,\n",
        "                                labels=labels)\n",
        "                loss = outputs[0]\n",
        "                logits = outputs[1]\n",
        "                _, pred = torch.max(logits.data, 1)\n",
        "                running_loss += loss.item()\n",
        "                total += labels.size(0)\n",
        "                correct += (pred == labels).sum().item()\n",
        "                \n",
        "            if predictions is None:\n",
        "                predictions = pred\n",
        "            else:\n",
        "                predictions = torch.cat((predictions, pred))\n",
        "    \n",
        "    if compute_acc:\n",
        "        acc = correct / total\n",
        "        loss = running_loss / total\n",
        "        return predictions, acc, loss\n",
        "    \n",
        "    return predictions\n",
        "    \n",
        "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# 這邊要記得確認 model 在 GPU 上運行 (投影片有說明)，否則會跑很久！\n",
        "print(\"device:\", device)\n",
        "model = model.to(device)\n",
        "#_, train_acc, train_loss = get_predictions(model, trainloader, compute_acc=True)\n",
        "#print(\"train acc:\", train_acc)\n",
        "#print(\"train loss:\", train_loss)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T17:37:12.274497Z",
          "iopub.execute_input": "2022-06-03T17:37:12.275070Z",
          "iopub.status.idle": "2022-06-03T17:37:12.402193Z",
          "shell.execute_reply.started": "2022-06-03T17:37:12.275026Z",
          "shell.execute_reply": "2022-06-03T17:37:12.401229Z"
        },
        "trusted": true,
        "id": "7fhcbb4SEN7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "EPOCHS = 3  # 由於時間有限，訓練 3 輪看看表現如何就好\n",
        "for epoch in range(EPOCHS):\n",
        "    running_loss = 0.0\n",
        "    for data in tqdm(trainloader):\n",
        "        \n",
        "        tokens_tensors, segments_tensors, \\\n",
        "        masks_tensors, labels = [t.to(device) for t in data]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(input_ids=tokens_tensors, \n",
        "                        token_type_ids=segments_tensors, \n",
        "                        attention_mask=masks_tensors, \n",
        "                        labels=labels)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # 紀錄當前 batch loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # 計算分類準確率\n",
        "    _, train_acc, train_loss = get_predictions(model, trainloader, compute_acc=True)\n",
        "    _, valid_acc, valid_loss = get_predictions(model, validloader, compute_acc=True)\n",
        "    \n",
        "    print('[epoch %d] train loss: %.3f, train acc: %.3f, valid loss: %.3f, valid acc: %.3f' %\n",
        "          (epoch + 1, train_loss, train_acc, valid_loss, valid_acc))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T17:37:12.405011Z",
          "iopub.execute_input": "2022-06-03T17:37:12.405986Z",
          "iopub.status.idle": "2022-06-03T18:19:21.155161Z",
          "shell.execute_reply.started": "2022-06-03T17:37:12.405926Z",
          "shell.execute_reply": "2022-06-03T18:19:21.154295Z"
        },
        "trusted": true,
        "id": "pAk79dlzEN7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), '/content/BDA/MyDrive/dataset/checkpoint_5530.pt')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:19:21.156891Z",
          "iopub.execute_input": "2022-06-03T18:19:21.157552Z",
          "iopub.status.idle": "2022-06-03T18:19:22.335301Z",
          "shell.execute_reply.started": "2022-06-03T18:19:21.157514Z",
          "shell.execute_reply": "2022-06-03T18:19:22.334155Z"
        },
        "trusted": true,
        "id": "MmE0QImPEN7s"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=NUM_LABELS)\n",
        "# model.load_state_dict(torch.load('/content/BDA/MyDrive/dataset/checkpoint_5530.pt'))"
      ],
      "metadata": {
        "id": "qhfjaKomq9-a"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testset = myDataset(\"test\", test, tokenizer=tokenizer)\n",
        "testloader = DataLoader(testset, batch_size=256, \n",
        "                        collate_fn=create_mini_batch)\n",
        "\n",
        "# 請我們的模型給出它的預測！\n",
        "predictions = get_predictions(model, testloader)\n",
        "# 要和在 cpu 上的 test_y 算準確度，還要把它從 GPU 上搬回來才行\n",
        "predictions = predictions.cpu().numpy()\n",
        "print(predictions)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:26:11.445230Z",
          "iopub.execute_input": "2022-06-03T18:26:11.445665Z",
          "iopub.status.idle": "2022-06-03T18:26:41.266134Z",
          "shell.execute_reply.started": "2022-06-03T18:26:11.445634Z",
          "shell.execute_reply": "2022-06-03T18:26:41.265178Z"
        },
        "trusted": true,
        "id": "z4EtERCREN7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = testset.df.copy()\n",
        "\n",
        "tt = test.copy()\n",
        "t1['pred'] = predictions\n",
        "tidx = t1.groupby(['prompt'])['pred'].apply(lambda x: x.value_counts().index[0]).reset_index()\n",
        "tt = pd.merge(tt, tidx, on=['prompt'])\n",
        "# tt.head(5)\n"
      ],
      "metadata": {
        "id": "4nhtLm-KRKmK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = ['', 'pred']\n",
        "rows = []\n",
        "for i in range(len(tt['pred'])):\n",
        "    rows.append((i, tt.iloc[i]['pred']))\n",
        "\n",
        "with open('/content/BDA/MyDrive/dataset/predictions_merged.csv','w') as f:\n",
        "    writeCsv = csv.writer(f)\n",
        "    writeCsv.writerow(headers)\n",
        "    writeCsv.writerows(rows)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-03T18:26:42.991100Z",
          "iopub.execute_input": "2022-06-03T18:26:42.991491Z",
          "iopub.status.idle": "2022-06-03T18:26:43.021147Z",
          "shell.execute_reply.started": "2022-06-03T18:26:42.991459Z",
          "shell.execute_reply": "2022-06-03T18:26:43.020347Z"
        },
        "trusted": true,
        "id": "ptcYs07aEN7t"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}